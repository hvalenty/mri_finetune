{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a350a6b6-4094-4051-b8bf-dfc6dfd0f179",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c81020-0acc-4fc0-875f-b2cba4e48fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataset.dataset import load_data\n",
    "from models import MRnet\n",
    "from config import config\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.utils import _train_model, _evaluate_model, _get_lr\n",
    "import time\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93ba76-e97f-41c5-a805-4b8f48c14782",
   "metadata": {
    "tags": []
   },
   "source": [
    "Method for training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d810f7c-83fb-45ff-9fe2-be3758cd99dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Performs training of a specified model.\n",
    "    \n",
    "Input params:\n",
    "    config_file: Takes in configurations to train with \n",
    "\"\"\"\n",
    "\n",
    "def train(config : dict):\n",
    "    \"\"\"\n",
    "    Function where actual training takes place\n",
    "\n",
    "    Args:\n",
    "        config (dict) : Configuration to train with\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Starting to Train Model...')\n",
    "\n",
    "    train_loader, val_loader, test_loader, train_wts, val_wts, test_wts = load_data()\n",
    "\n",
    "    print('Initializing Model...')\n",
    "    model = MRnet()\n",
    "    # Load the weights from the previous model\n",
    "    checkpoint = torch.load(\"weights/acl/model_test_acl_val_auc_0.9677_train_auc_0.9903_epoch_20.pth\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "   # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace final classification head with a new one (e.g., two layers)\n",
    "    num_features = model.fc[0].in_features if isinstance(model.fc, nn.Sequential) else model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_features, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 4)\n",
    "    )\n",
    "\n",
    "    # Unfreeze only the last two layers (ReLU has no parameters, so we unfreeze both Linear layers)\n",
    "    for param in model.fc[0].parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.fc[2].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        train_wts = train_wts.cuda()\n",
    "        val_wts = val_wts.cuda()\n",
    "\n",
    "    print('Initializing Loss Method...')\n",
    "    criterion = nn.CrossEntropyLoss(weight=train_wts)\n",
    "    val_criterion = nn.CrossEntropyLoss(weight=val_wts)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        criterion = criterion.cuda()\n",
    "        val_criterion = val_criterion.cuda()\n",
    "\n",
    "    print('Setup the Optimizer')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, patience=3, factor=.3, threshold=1e-4, verbose=True)\n",
    "    \n",
    "    starting_epoch = config['starting_epoch']\n",
    "    num_epochs = config['max_epoch']\n",
    "    patience = config['patience']\n",
    "    log_train = config['log_train']\n",
    "    log_val = config['log_val']\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_auc = float(0)\n",
    "\n",
    "    print('Starting Training')\n",
    "\n",
    "    writer = SummaryWriter(comment='lr={} task=acl'.format(config['lr']))\n",
    "    t_start_training = time.time()\n",
    "\n",
    "    for epoch in range(starting_epoch, num_epochs):\n",
    "\n",
    "        current_lr = _get_lr(optimizer)\n",
    "        epoch_start_time = time.time()  # timer for entire epoch\n",
    "\n",
    "        print('Started Training')\n",
    "        train_loss, train_auc = _train_model(\n",
    "            model, train_loader, epoch, num_epochs, optimizer, criterion, writer, current_lr, log_every=log_train)\n",
    "\n",
    "        print('train loop ended, now val')\n",
    "        val_loss, val_auc = _evaluate_model(\n",
    "            model, val_loader, val_criterion,  epoch, num_epochs, writer, current_lr, log_val)\n",
    "\n",
    "        writer.add_scalar('Train/Avg Loss', train_loss, epoch)\n",
    "        writer.add_scalar('Val/Avg Loss', val_loss, epoch)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        t_end = time.time()\n",
    "        delta = t_end - epoch_start_time\n",
    "\n",
    "        print(\"train loss : {0} | train auc {1} | val loss {2} | val auc {3} | elapsed time {4} s\".format(\n",
    "            train_loss, train_auc, val_loss, val_auc, delta))\n",
    "\n",
    "        print('-' * 30)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "\n",
    "        if bool(config['save_model']) and (epoch+1) % 10 == 0:\n",
    "            file_name = 'model_{}_{}_val_auc_{:0.4f}_train_auc_{:0.4f}_epoch_{}.pth'.format(config['exp_name'], config['task'], val_auc, train_auc, epoch+1)\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict()\n",
    "            }, './weights/{}/{}'.format(config['task'],file_name))\n",
    "\n",
    "    t_end_training = time.time()\n",
    "    print(f'training took {t_end_training - t_start_training} s')\n",
    "    writer.flush()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8eecb-4508-415c-a6b3-a507c23b76ff",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19133bc4-876f-4f68-8aa8-83be1e5895d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration\n",
      "{'max_epoch': 50, 'log_train': 100, 'lr': 1e-05, 'starting_epoch': 0, 'batch_size': 1, 'log_val': 10, 'weight_decay': 0.01, 'patience': 5, 'save_model': 1, 'exp_name': 'test'}\n",
      "Starting to Train Model...\n",
      "Loading Train Dataset of ACL task...\n",
      "['001', '008', '015', '016', '084', '098', '101', '107', '109', '124', '129', '145', '150', '164', '172', '183', '201', '209', '225', '230', '245', '251']\n",
      "Unique labels found in dataset: [0, 1, 2, 3]\n",
      "Number of classes: 4\n",
      "Class distribution:\n",
      "Class 0: 7 samples\n",
      "Class 1: 3 samples\n",
      "Class 2: 8 samples\n",
      "Class 3: 4 samples\n",
      "Class weights for loss are: tensor([0.6713, 1.5664, 0.5874, 1.1748])\n",
      "Total samples: 22 | Num classes: 4\n",
      "Loading Validation Dataset of ACL task...\n",
      "['037', '062', '133', '184', '207', '121']\n",
      "Unique labels found in dataset: [0, 1, 2, 3]\n",
      "Number of classes: 4\n",
      "Class distribution:\n",
      "Class 0: 2 samples\n",
      "Class 1: 1 samples\n",
      "Class 2: 1 samples\n",
      "Class 3: 2 samples\n",
      "Class weights for loss are: tensor([0.6667, 1.3333, 1.3333, 0.6667])\n",
      "Total samples: 6 | Num classes: 4\n",
      "Loading Testing Dataset of ACL task...\n",
      "['006', '030', '034', '048', '052', '079', '099', '126', '158', '176', '178', '196', '219', '237', '244']\n",
      "Unique labels found in dataset: [0, 1, 2, 3]\n",
      "Number of classes: 4\n",
      "Class distribution:\n",
      "Class 0: 4 samples\n",
      "Class 1: 1 samples\n",
      "Class 2: 2 samples\n",
      "Class 3: 8 samples\n",
      "Class weights for loss are: tensor([0.5333, 2.1333, 1.0667, 0.2667])\n",
      "Total samples: 15 | Num classes: 4\n",
      "Initializing Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qdy4zt/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/qdy4zt/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Loss Method...\n",
      "Setup the Optimizer\n",
      "Starting Training\n",
      "Started Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qdy4zt/.local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('Training Configuration')\n",
    "print(config)\n",
    "\n",
    "train(config=config)\n",
    "\n",
    "print('Training Ended...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef1589-8889-470c-8d31-8537b3c3ba03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e83313-fb75-4d31-a07e-85c854af6cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
