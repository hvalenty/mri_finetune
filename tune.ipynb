{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a350a6b6-4094-4051-b8bf-dfc6dfd0f179",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c81020-0acc-4fc0-875f-b2cba4e48fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataset.dataset import load_data\n",
    "from models import MRnet\n",
    "from config import config\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.utils import _train_model, _evaluate_model, _get_lr\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93ba76-e97f-41c5-a805-4b8f48c14782",
   "metadata": {
    "tags": []
   },
   "source": [
    "Method for training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d810f7c-83fb-45ff-9fe2-be3758cd99dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(config: dict):\n",
    "    \"\"\"\n",
    "    Function where actual fine-tuning takes place\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration to train with\n",
    "    \"\"\"\n",
    "\n",
    "    print('Starting to Train Model...')\n",
    "\n",
    "    train_loader, val_loader, test_loader, train_wts, val_wts, test_wts = load_data()\n",
    "\n",
    "    print('Initializing Model...')\n",
    "    model = MRnet()\n",
    "\n",
    "    # Load the weights from the pretrained binary model\n",
    "    checkpoint = torch.load(\"weights/acl/model_test_acl_val_auc_0.9677_train_auc_0.9903_epoch_20.pth\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    # --------- FINE-TUNING STRATEGY ---------\n",
    "\n",
    "    # Freeze all layers initially\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze later convolutional layers for each axis\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(layer in name for layer in ['axial.10', 'coronal.10', 'saggital.10', 'axial.8', 'coronal.8', 'saggital.8']):\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Replace the final fully connected layer for 4-class output with Dropout and LayerNorm\n",
    "    num_features = model.fc[0].in_features if isinstance(model.fc, torch.nn.Sequential) else model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_features, 128),\n",
    "        nn.LayerNorm(128),  # Added LayerNorm\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),  # Added Dropout\n",
    "        nn.Linear(128, 4)  # 4-class severity grading\n",
    "    )\n",
    "\n",
    "    # Make sure the new fc layers are trainable\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        train_wts = train_wts.cuda()\n",
    "        val_wts = val_wts.cuda()\n",
    "\n",
    "    print('Initializing Loss Method...')\n",
    "    criterion = nn.CrossEntropyLoss(weight=train_wts)\n",
    "    val_criterion = nn.CrossEntropyLoss(weight=val_wts)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        criterion = criterion.cuda()\n",
    "        val_criterion = val_criterion.cuda()\n",
    "\n",
    "    print('Setup the Optimizer')\n",
    "    # Only optimize trainable parameters\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, patience=3, factor=0.3, threshold=1e-4, verbose=True\n",
    "    )\n",
    "\n",
    "    starting_epoch = config['starting_epoch']\n",
    "    num_epochs = config['max_epoch']\n",
    "    log_train = config['log_train']\n",
    "    log_val = config['log_val']\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = float(0)\n",
    "    patience_counter = 0\n",
    "    early_stopping_patience = 10  # Example patience for early stopping\n",
    "    best_model_state = None # To store the best model's state\n",
    "\n",
    "    print('Starting Training')\n",
    "\n",
    "    writer = SummaryWriter(comment=f\"lr={config['lr']} task=acl-grading\")\n",
    "    t_start_training = time.time()\n",
    "\n",
    "    for epoch in range(starting_epoch, num_epochs):\n",
    "\n",
    "        current_lr = _get_lr(optimizer)\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        print(f'Starting Epoch {epoch + 1}/{num_epochs}')\n",
    "        train_loss, train_acc = _train_model(\n",
    "            model, train_loader, epoch, num_epochs, optimizer, criterion, writer, current_lr, log_every=log_train\n",
    "        )\n",
    "\n",
    "        print('Train loop ended, now evaluating on validation set...')\n",
    "        val_loss, val_acc = _evaluate_model(\n",
    "            model, val_loader, val_criterion, epoch, num_epochs, writer, current_lr, log_val\n",
    "        )\n",
    "\n",
    "        writer.add_scalar('Train/Avg Loss', train_loss, epoch)\n",
    "        writer.add_scalar('Val/Avg Loss', val_loss, epoch)\n",
    "        writer.add_scalar('Train/Avg Accuracy', train_acc, epoch)\n",
    "        writer.add_scalar('Val/Avg Accuracy', val_acc, epoch)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n",
    "              f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        print('-' * 50)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc = val_acc # Update best val accuracy when loss improves\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state) # Load the best model\n",
    "                break\n",
    "\n",
    "        if bool(config['save_model']) and (epoch + 1) % 10 == 0:\n",
    "            file_name = f\"model_{config['exp_name']}_acl_val_acc_{val_acc:.4f}_train_acc_{train_acc:.4f}_epoch_{epoch + 1}.pth\"\n",
    "            save_path = os.path.join('weights', \"acl\", file_name)\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            torch.save({'model_state_dict': model.state_dict()}, save_path)\n",
    "\n",
    "    t_end_training = time.time()\n",
    "    print(f'Training completed in {t_end_training - t_start_training:.2f}s')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8eecb-4508-415c-a6b3-a507c23b76ff",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19133bc4-876f-4f68-8aa8-83be1e5895d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Training Configuration')\n",
    "print(config)\n",
    "\n",
    "train(config=config)\n",
    "\n",
    "print('Training Ended...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef1589-8889-470c-8d31-8537b3c3ba03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e83313-fb75-4d31-a07e-85c854af6cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
