{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a350a6b6-4094-4051-b8bf-dfc6dfd0f179",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c81020-0acc-4fc0-875f-b2cba4e48fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataset.dataset import load_data\n",
    "from models import MRnet\n",
    "from config import config\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.utils import _train_model, _evaluate_model, _get_lr\n",
    "import time\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93ba76-e97f-41c5-a805-4b8f48c14782",
   "metadata": {
    "tags": []
   },
   "source": [
    "Method for training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d810f7c-83fb-45ff-9fe2-be3758cd99dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Performs training of a specified model.\n",
    "    \n",
    "Input params:\n",
    "    config_file: Takes in configurations to train with \n",
    "\"\"\"\n",
    "\n",
    "def train(config : dict):\n",
    "    \"\"\"\n",
    "    Function where actual training takes place\n",
    "\n",
    "    Args:\n",
    "        config (dict) : Configuration to train with\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Starting to Train Model...')\n",
    "\n",
    "    train_loader, val_loader, test_loader, train_wts, val_wts, test_wts = load_data()\n",
    "\n",
    "    print('Initializing Model...')\n",
    "    model = MRnet()\n",
    "    # Load the weights from the previous model\n",
    "    checkpoint = torch.load(\"weights/acl/model_test_acl_val_auc_0.9677_train_auc_0.9903_epoch_20.pth\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "   # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace final classification head with a new one (e.g., two layers)\n",
    "    num_features = model.fc[0].in_features if isinstance(model.fc, nn.Sequential) else model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_features, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 4)\n",
    "    )\n",
    "\n",
    "    # Unfreeze only the last two layers (ReLU has no parameters, so we unfreeze both Linear layers)\n",
    "    for param in model.fc[0].parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.fc[2].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        train_wts = train_wts.cuda()\n",
    "        val_wts = val_wts.cuda()\n",
    "\n",
    "    print('Initializing Loss Method...')\n",
    "    criterion = nn.CrossEntropyLoss(weight=train_wts)\n",
    "    val_criterion = nn.CrossEntropyLoss(weight=val_wts)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        criterion = criterion.cuda()\n",
    "        val_criterion = val_criterion.cuda()\n",
    "\n",
    "    print('Setup the Optimizer')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, patience=3, factor=.3, threshold=1e-4, verbose=True)\n",
    "    \n",
    "    starting_epoch = config['starting_epoch']\n",
    "    num_epochs = config['max_epoch']\n",
    "    patience = config['patience']\n",
    "    log_train = config['log_train']\n",
    "    log_val = config['log_val']\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_auc = float(0)\n",
    "\n",
    "    print('Starting Training')\n",
    "\n",
    "    writer = SummaryWriter(comment='lr={} task=acl'.format(config['lr']))\n",
    "    t_start_training = time.time()\n",
    "\n",
    "    for epoch in range(starting_epoch, num_epochs):\n",
    "\n",
    "        current_lr = _get_lr(optimizer)\n",
    "        epoch_start_time = time.time()  # timer for entire epoch\n",
    "\n",
    "        print('Started Training')\n",
    "        train_loss, train_auc = _train_model(\n",
    "            model, train_loader, epoch, num_epochs, optimizer, criterion, writer, current_lr, log_every=log_train)\n",
    "\n",
    "        print('train loop ended, now val')\n",
    "        val_loss, val_auc = _evaluate_model(\n",
    "            model, val_loader, val_criterion,  epoch, num_epochs, writer, current_lr, log_val)\n",
    "\n",
    "        writer.add_scalar('Train/Avg Loss', train_loss, epoch)\n",
    "        writer.add_scalar('Val/Avg Loss', val_loss, epoch)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        t_end = time.time()\n",
    "        delta = t_end - epoch_start_time\n",
    "\n",
    "        print(\"train loss : {0} | train auc {1} | val loss {2} | val auc {3} | elapsed time {4} s\".format(\n",
    "            train_loss, train_auc, val_loss, val_auc, delta))\n",
    "\n",
    "        print('-' * 30)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "\n",
    "        if bool(config['save_model']) and (epoch+1) % 10 == 0:\n",
    "            file_name = 'model_{}_{}_val_auc_{:0.4f}_train_auc_{:0.4f}_epoch_{}.pth'.format(config['exp_name'], config['task'], val_auc, train_auc, epoch+1)\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict()\n",
    "            }, './weights/{}/{}'.format(config['task'],file_name))\n",
    "\n",
    "    t_end_training = time.time()\n",
    "    print(f'training took {t_end_training - t_start_training} s')\n",
    "    writer.flush()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8eecb-4508-415c-a6b3-a507c23b76ff",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19133bc4-876f-4f68-8aa8-83be1e5895d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration\n",
      "{'max_epoch': 50, 'log_train': 100, 'lr': 1e-05, 'starting_epoch': 0, 'batch_size': 1, 'log_val': 10, 'weight_decay': 0.01, 'patience': 5, 'save_model': 1, 'exp_name': 'test'}\n",
      "Starting to Train Model...\n",
      "Loading Train Dataset of ACL task...\n",
      "Class distribution:\n",
      "Class 0: 7 samples\n",
      "Class 1: 3 samples\n",
      "Class 2: 8 samples\n",
      "Class 3: 4 samples\n",
      "Class 4: 1 samples\n",
      "Class weights for loss are: tensor([0.3859, 0.9003, 0.3376, 0.6752, 2.7010])\n",
      "Total samples: 23 | Num classes: 5\n",
      "Loading Validation Dataset of ACL task...\n",
      "Class distribution:\n",
      "Class 0: 1 samples\n",
      "Class weights for loss are: tensor([1.])\n",
      "Total samples: 7 | Num classes: 1\n",
      "Loading Testing Dataset of ACL task...\n",
      "Class distribution:\n",
      "Class 0: 1 samples\n",
      "Class weights for loss are: tensor([1.])\n",
      "Total samples: 16 | Num classes: 1\n",
      "Initializing Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qdy4zt/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 11 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/qdy4zt/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 11 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/qdy4zt/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 11 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/qdy4zt/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/qdy4zt/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Loss Method...\n",
      "Setup the Optimizer\n",
      "Starting Training\n",
      "Started Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qdy4zt/.local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/qdy4zt/.local/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/qdy4zt/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/qdy4zt/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/sfs/gpfs/tardis/home/qdy4zt/Coding Projects/MSDS/Deep Learning/alex_net/dataset/dataset.py\", line 119, in __getitem__\n    label = torch.tensor(self.labels[index]).long()\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: new(): invalid data type 'str'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Configuration\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(config)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Ended...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 82\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     79\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# timer for entire epoch\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarted Training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m train_loss, train_auc \u001b[38;5;241m=\u001b[39m \u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain loop ended, now val\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m val_loss, val_auc \u001b[38;5;241m=\u001b[39m _evaluate_model(\n\u001b[1;32m     87\u001b[0m     model, val_loader, val_criterion,  epoch, num_epochs, writer, current_lr, log_val)\n",
      "File \u001b[0;32m/sfs/gpfs/tardis/home/qdy4zt/Coding Projects/MSDS/Deep Learning/alex_net/utils/utils.py:64\u001b[0m, in \u001b[0;36m_train_model\u001b[0;34m(model, train_loader, epoch, num_epochs, optimizer, criterion, writer, current_lr, log_every)\u001b[0m\n\u001b[1;32m     61\u001b[0m y_gt \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     62\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     65\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/qdy4zt/.local/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/qdy4zt/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/qdy4zt/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/sfs/gpfs/tardis/home/qdy4zt/Coding Projects/MSDS/Deep Learning/alex_net/dataset/dataset.py\", line 119, in __getitem__\n    label = torch.tensor(self.labels[index]).long()\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: new(): invalid data type 'str'\n"
     ]
    }
   ],
   "source": [
    "print('Training Configuration')\n",
    "print(config)\n",
    "\n",
    "train(config=config)\n",
    "\n",
    "print('Training Ended...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef1589-8889-470c-8d31-8537b3c3ba03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e83313-fb75-4d31-a07e-85c854af6cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
